{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CultureScope: Cultural Specificity Classification\n",
    "\n",
    "**Task:** Classify items into three cultural specificity categories:\n",
    "- `cultural agnostic` - Universally known, no specific cultural ownership\n",
    "- `cultural representative` - Associated with a culture but known globally\n",
    "- `cultural exclusive` - Known primarily within a specific culture\n",
    "\n",
    "**Model:** XGBoost with Wikipedia/Wikidata feature extraction\n",
    "\n",
    "**HuggingFace Model:** [ArchitRastogi/CultureScope-XGBoost](https://huggingface.co/ArchitRastogi/CultureScope-XGBoost)\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "- **Part 1:** Full pipeline with Wikipedia feature extraction at inference time\n",
    "- **Part 2:** Fallback approach without external API calls\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q datasets huggingface_hub joblib xgboost lightgbm catboost scikit-learn pandas numpy requests hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import joblib\n",
    "import pickle\n",
    "from typing import Dict, Optional, List\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download, login, whoami\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- CONFIGURATION -----\n",
    "\n",
    "# Input: Choose one of the following options\n",
    "USE_HUGGINGFACE_DATASET = True  # Set to False to use local CSV file\n",
    "INPUT_CSV_PATH = \"test.csv\"     # Path to input CSV (used if USE_HUGGINGFACE_DATASET=False)\n",
    "\n",
    "# HuggingFace dataset configuration\n",
    "HF_DATASET_NAME = \"sapienzanlp/nlp2025_hw1_cultural_dataset\"  # Dataset name on HuggingFace\n",
    "HF_DATASET_SPLIT = \"Enter split name\"                               # Split to use\n",
    "\n",
    "if USE_HUGGINGFACE_DATASET:\n",
    "    HF_TOKEN = \"\"  # Add HuggingFace token here, as the datatset is gated\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"Logged in as:\", whoami())\n",
    "\n",
    "# Model configuration\n",
    "HF_MODEL_REPO = \"ArchitRastogi/CultureScope-XGBoost\"  # Model repository for Part 1 \n",
    "MODEL_FILENAME = \"best_model.pkl\"                     # Model file name\n",
    "\n",
    "HF_MODEL_REPO_PART2 =  \"ArchitRastogi/CultureScope-Ensemble-NoWiki\"\n",
    "MODEL_FILENAME_PART2 =  \"ensemble_simple.pkl\"\n",
    "\n",
    "# Output configuration\n",
    "OUTPUT_CSV_PATH = \"predictions.csv\"  # Output file path\n",
    "\n",
    "# Feature extraction settings\n",
    "MAX_WORKERS = 16        # Parallel workers for API calls\n",
    "REQUEST_TIMEOUT = 10    # API request timeout in seconds\n",
    "\n",
    "print(f\"Input: {'HuggingFace Dataset' if USE_HUGGINGFACE_DATASET else INPUT_CSV_PATH}\")\n",
    "print(f\"Output: {OUTPUT_CSV_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from HuggingFace: sapienzanlp/nlp2025_hw1_cultural_dataset\n",
      "Loaded 300 samples.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>type</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.wikidata.org/entity/Q15786</td>\n",
       "      <td>1. FC Nürnberg</td>\n",
       "      <td>German sports club based in Nuremberg, Bavaria</td>\n",
       "      <td>entity</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports club</td>\n",
       "      <td>cultural representative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.wikidata.org/entity/Q268530</td>\n",
       "      <td>77 Records</td>\n",
       "      <td>UK record label</td>\n",
       "      <td>entity</td>\n",
       "      <td>music</td>\n",
       "      <td>record label</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.wikidata.org/entity/Q216153</td>\n",
       "      <td>A Bug's Life</td>\n",
       "      <td>1998 animated film directed by John Lasseter a...</td>\n",
       "      <td>entity</td>\n",
       "      <td>comics and anime</td>\n",
       "      <td>animated film</td>\n",
       "      <td>cultural representative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.wikidata.org/entity/Q593</td>\n",
       "      <td>A Gang Story</td>\n",
       "      <td>2011 film by Olivier Marchal</td>\n",
       "      <td>entity</td>\n",
       "      <td>films</td>\n",
       "      <td>film</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.wikidata.org/entity/Q192185</td>\n",
       "      <td>Aaron Copland</td>\n",
       "      <td>American composer, composition teacher, writer...</td>\n",
       "      <td>entity</td>\n",
       "      <td>performing arts</td>\n",
       "      <td>choreographer</td>\n",
       "      <td>cultural representative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     item            name  \\\n",
       "0   http://www.wikidata.org/entity/Q15786  1. FC Nürnberg   \n",
       "1  http://www.wikidata.org/entity/Q268530      77 Records   \n",
       "2  http://www.wikidata.org/entity/Q216153    A Bug's Life   \n",
       "3     http://www.wikidata.org/entity/Q593    A Gang Story   \n",
       "4  http://www.wikidata.org/entity/Q192185   Aaron Copland   \n",
       "\n",
       "                                         description    type  \\\n",
       "0     German sports club based in Nuremberg, Bavaria  entity   \n",
       "1                                    UK record label  entity   \n",
       "2  1998 animated film directed by John Lasseter a...  entity   \n",
       "3                       2011 film by Olivier Marchal  entity   \n",
       "4  American composer, composition teacher, writer...  entity   \n",
       "\n",
       "           category    subcategory                    label  \n",
       "0            sports    sports club  cultural representative  \n",
       "1             music   record label       cultural exclusive  \n",
       "2  comics and anime  animated film  cultural representative  \n",
       "3             films           film       cultural exclusive  \n",
       "4   performing arts  choreographer  cultural representative  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_test_data(use_hf: bool = True, csv_path: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load test data from HuggingFace Hub or local CSV file.\n",
    "    \n",
    "    Args:\n",
    "        use_hf: If True, load from HuggingFace Hub\n",
    "        csv_path: Path to local CSV file (used if use_hf=False)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with test data\n",
    "    \"\"\"\n",
    "    if use_hf:\n",
    "        print(f\"Loading dataset from HuggingFace: {HF_DATASET_NAME}\")\n",
    "        dataset = load_dataset(HF_DATASET_NAME, split=HF_DATASET_SPLIT,token=HF_TOKEN,)\n",
    "        df = dataset.to_pandas()\n",
    "    else:\n",
    "        print(f\"Loading dataset from CSV: {csv_path}\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    required_cols = ['item', 'name', 'description', 'type', 'category', 'subcategory']\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = ''\n",
    "    \n",
    "    # Fill missing values\n",
    "    df = df.fillna('')\n",
    "    \n",
    "    print(f\"Loaded {len(df)} samples.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load test data\n",
    "test_df = load_test_data(use_hf=USE_HUGGINGFACE_DATASET, csv_path=INPUT_CSV_PATH)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: With Wikipedia Feature Extraction\n",
    "\n",
    "This approach extracts features from Wikipedia and Wikidata APIs at inference time to enrich the input data with cultural and geographic metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Feature Extraction Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WikiDataExtractor class defined.\n"
     ]
    }
   ],
   "source": [
    "class WikiDataExtractor:\n",
    "    \"\"\"\n",
    "    Extract features from Wikipedia and Wikidata for cultural classification.\n",
    "    \n",
    "    Features extracted:\n",
    "    - Wikipedia: language count, page length, categories, external links\n",
    "    - Wikidata: statements, cultural properties, geographic properties\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cultural property IDs in Wikidata\n",
    "    CULTURAL_PROPERTIES = {\n",
    "        'P17', 'P495', 'P2596', 'P27', 'P1412', 'P37',\n",
    "        'P103', 'P361', 'P131', 'P625'\n",
    "    }\n",
    "    \n",
    "    # Geographic property IDs in Wikidata\n",
    "    GEOGRAPHIC_PROPERTIES = {\n",
    "        'P17', 'P131', 'P625', 'P30', 'P47', 'P150',\n",
    "        'P36', 'P1376'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, timeout: int = 10):\n",
    "        self.timeout = timeout\n",
    "        self.session = requests.Session()\n",
    "        # Set proper User-Agent header\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'CulturalClassifier/1.0 (Educational Research Project; Python/requests)'\n",
    "        })\n",
    "    \n",
    "    def extract_features(self, item_name: str, wikidata_id: str = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract all features for a single item.\n",
    "        \n",
    "        Args:\n",
    "            item_name: Name of the item to search\n",
    "            wikidata_id: Optional Wikidata ID (e.g., 'Q12345')\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of extracted features\n",
    "        \"\"\"\n",
    "        features = self._get_default_features()\n",
    "        \n",
    "        try:\n",
    "            # Extract Wikipedia features\n",
    "            wiki_features = self._get_wikipedia_features(item_name)\n",
    "            features.update(wiki_features)\n",
    "            \n",
    "            # Extract Wikidata features\n",
    "            qid = wikidata_id or self._search_wikidata(item_name)\n",
    "            if qid:\n",
    "                wikidata_features = self._get_wikidata_features(qid)\n",
    "                features.update(wikidata_features)\n",
    "        except Exception as e:\n",
    "            # Silently fail and return defaults\n",
    "            pass\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _get_default_features(self) -> Dict:\n",
    "        \"\"\"Return default feature values.\"\"\"\n",
    "        return {\n",
    "            'num_languages': 0,\n",
    "            'en_page_length': 0,\n",
    "            'num_categories': 0,\n",
    "            'num_external_links': 0,\n",
    "            'has_coordinates': 0,\n",
    "            'num_statements': 0,\n",
    "            'statement_diversity': 0,\n",
    "            'num_cultural_properties': 0,\n",
    "            'num_geographic_properties': 0,\n",
    "            'has_country': 0,\n",
    "            'has_origin_country': 0,\n",
    "            'has_culture_property': 0,\n",
    "            'num_identifiers': 0\n",
    "        }\n",
    "    \n",
    "    def _get_wikipedia_features(self, title: str) -> Dict:\n",
    "        \"\"\"Extract features from Wikipedia API.\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Get language links count\n",
    "        url = \"https://en.wikipedia.org/w/api.php\"\n",
    "        params = {\n",
    "            'action': 'query',\n",
    "            'titles': title,\n",
    "            'prop': 'langlinks|categories|extlinks|coordinates|info',\n",
    "            'lllimit': 'max',\n",
    "            'cllimit': 'max',\n",
    "            'ellimit': 'max',\n",
    "            'format': 'json'\n",
    "        }\n",
    "        \n",
    "        response = self.session.get(url, params=params, timeout=self.timeout)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        pages = data.get('query', {}).get('pages', {})\n",
    "        if pages:\n",
    "            page = list(pages.values())[0]\n",
    "            if 'missing' not in page:\n",
    "                features['num_languages'] = len(page.get('langlinks', []))\n",
    "                features['en_page_length'] = page.get('length', 0)\n",
    "                features['num_categories'] = len(page.get('categories', []))\n",
    "                features['num_external_links'] = len(page.get('extlinks', []))\n",
    "                features['has_coordinates'] = 1 if 'coordinates' in page else 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _search_wikidata(self, query: str) -> Optional[str]:\n",
    "        \"\"\"Search for Wikidata entity ID.\"\"\"\n",
    "        url = \"https://www.wikidata.org/w/api.php\"\n",
    "        params = {\n",
    "            'action': 'wbsearchentities',\n",
    "            'search': query,\n",
    "            'language': 'en',\n",
    "            'limit': 1,\n",
    "            'format': 'json'\n",
    "        }\n",
    "        \n",
    "        response = self.session.get(url, params=params, timeout=self.timeout)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        results = data.get('search', [])\n",
    "        return results[0]['id'] if results else None\n",
    "    \n",
    "    def _get_wikidata_features(self, qid: str) -> Dict:\n",
    "        \"\"\"Extract features from Wikidata API.\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        url = f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\"\n",
    "        response = self.session.get(url, timeout=self.timeout)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        entity = data.get('entities', {}).get(qid, {})\n",
    "        claims = entity.get('claims', {})\n",
    "        \n",
    "        # Count statements and properties\n",
    "        all_properties = set(claims.keys())\n",
    "        features['num_statements'] = sum(len(v) for v in claims.values())\n",
    "        features['statement_diversity'] = len(all_properties)\n",
    "        \n",
    "        # Cultural and geographic properties\n",
    "        features['num_cultural_properties'] = len(all_properties & self.CULTURAL_PROPERTIES)\n",
    "        features['num_geographic_properties'] = len(all_properties & self.GEOGRAPHIC_PROPERTIES)\n",
    "        \n",
    "        # Specific property flags\n",
    "        features['has_country'] = 1 if 'P17' in claims else 0\n",
    "        features['has_origin_country'] = 1 if 'P495' in claims else 0\n",
    "        features['has_culture_property'] = 1 if 'P2596' in claims else 0\n",
    "        \n",
    "        # Count identifier properties (P prefixes in certain ranges)\n",
    "        identifiers = [p for p in all_properties if p.startswith('P') and \n",
    "                      p[1:].isdigit() and int(p[1:]) > 200]\n",
    "        features['num_identifiers'] = len(identifiers)\n",
    "        \n",
    "        return features\n",
    "\n",
    "\n",
    "print(\"WikiDataExtractor class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Extract Features for Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for 300 items (workers=16)...\n",
      "  Progress: 50/300 (50.3 items/sec)\n",
      "  Progress: 100/300 (63.5 items/sec)\n",
      "  Progress: 150/300 (70.6 items/sec)\n",
      "  Progress: 200/300 (73.0 items/sec)\n",
      "  Progress: 250/300 (74.4 items/sec)\n",
      "  Progress: 300/300 (75.9 items/sec)\n",
      "Feature extraction complete. Time: 4.0s\n",
      "\n",
      "Testing: A Bug's Life\n",
      "Item URL: http://www.wikidata.org/entity/Q216153\n",
      "QID: Q216153\n",
      "\n",
      "Extracted features:\n",
      "  num_languages: 63\n",
      "  en_page_length: 82095\n",
      "  num_categories: 38\n",
      "  num_external_links: 169\n",
      "  num_statements: 194\n",
      "  statement_diversity: 132\n",
      "  num_cultural_properties: 1\n",
      "  has_origin_country: 1\n",
      "  num_identifiers: 122\n"
     ]
    }
   ],
   "source": [
    "def extract_features_parallel(df: pd.DataFrame, max_workers: int = 16) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract Wikipedia/Wikidata features for all items in parallel.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'name' and optionally 'item' columns\n",
    "        max_workers: Number of parallel workers\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with extracted features\n",
    "    \"\"\"\n",
    "    extractor = WikiDataExtractor(timeout=REQUEST_TIMEOUT)\n",
    "    results = []\n",
    "    \n",
    "    def process_row(idx, row):\n",
    "        item_name = row['name']\n",
    "        wikidata_id = None\n",
    "        \n",
    "        # Extract Wikidata ID from item URI if available\n",
    "        if 'item' in row and row['item']:\n",
    "            item_str = str(row['item'])\n",
    "            if 'wikidata.org' in item_str:\n",
    "                wikidata_id = item_str.split('/')[-1]\n",
    "        \n",
    "        features = extractor.extract_features(item_name, wikidata_id)\n",
    "        features['_idx'] = idx\n",
    "        return features\n",
    "    \n",
    "    print(f\"Extracting features for {len(df)} items (workers={max_workers})...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(process_row, idx, row): idx\n",
    "            for idx, row in df.iterrows()\n",
    "        }\n",
    "        \n",
    "        for i, future in enumerate(as_completed(futures)):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                idx = futures[future]\n",
    "                results.append({'_idx': idx, **extractor._get_default_features()})\n",
    "            \n",
    "            # Progress update every 50 items\n",
    "            if (i + 1) % 50 == 0 or (i + 1) == len(df):\n",
    "                elapsed = time.time() - start_time\n",
    "                rate = (i + 1) / elapsed\n",
    "                print(f\"  Progress: {i+1}/{len(df)} ({rate:.1f} items/sec)\")\n",
    "    \n",
    "    # Convert to DataFrame and sort by original index\n",
    "    features_df = pd.DataFrame(results).sort_values('_idx').drop('_idx', axis=1)\n",
    "    features_df.index = df.index\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Feature extraction complete. Time: {elapsed:.1f}s\")\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "\n",
    "# Extract raw features\n",
    "raw_features_df = extract_features_parallel(test_df, max_workers=MAX_WORKERS)\n",
    "# Test if API extraction works for ONE item\n",
    "sample_item = test_df.iloc[2]  # A Bug's Life\n",
    "print(f\"\\nTesting: {sample_item['name']}\")\n",
    "print(f\"Item URL: {sample_item['item']}\")\n",
    "\n",
    "extractor = WikiDataExtractor(timeout=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Feature Engineering Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered features shape: (300, 33)\n"
     ]
    }
   ],
   "source": [
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Engineer features EXACTLY like training / tuning.\n",
    "    \n",
    "    Expects raw Wikipedia/Wikidata columns from extract_features_parallel.\n",
    "    \"\"\"\n",
    "    feature_df = df.copy()\n",
    "\n",
    "    # Ensure required raw columns exist\n",
    "    for col in [\n",
    "        'num_languages', 'en_page_length', 'num_categories', 'num_external_links',\n",
    "        'num_statements', 'num_identifiers', 'statement_diversity',\n",
    "        'num_cultural_properties', 'num_geographic_properties',\n",
    "        'has_coordinates', 'has_country', 'has_culture_property', 'has_origin_country'\n",
    "    ]:\n",
    "        if col not in feature_df.columns:\n",
    "            feature_df[col] = 0\n",
    "\n",
    "    # ------- Log transforms -------\n",
    "    feature_df['log_num_languages']      = np.log1p(feature_df['num_languages'])\n",
    "    feature_df['log_en_page_length']     = np.log1p(feature_df['en_page_length'])\n",
    "    feature_df['log_num_statements']     = np.log1p(feature_df['num_statements'])\n",
    "    feature_df['log_num_categories']     = np.log1p(feature_df['num_categories'])\n",
    "    feature_df['log_num_external_links'] = np.log1p(feature_df['num_external_links'])\n",
    "    feature_df['log_num_identifiers']    = np.log1p(feature_df['num_identifiers'])\n",
    "    feature_df['log_statement_diversity'] = np.log1p(feature_df['statement_diversity'])\n",
    "\n",
    "    # ------- Ratio features -------\n",
    "    feature_df['cultural_ratio']   = feature_df['num_cultural_properties'] / (feature_df['statement_diversity'] + 1)\n",
    "    feature_df['geographic_ratio'] = feature_df['num_geographic_properties'] / (feature_df['statement_diversity'] + 1)\n",
    "    feature_df['identifier_ratio'] = feature_df['num_identifiers'] / (feature_df['num_statements'] + 1)\n",
    "    feature_df['categories_per_page']    = feature_df['num_categories'] / (feature_df['en_page_length'] + 1)\n",
    "    feature_df['external_links_per_page'] = feature_df['num_external_links'] / (feature_df['en_page_length'] + 1)\n",
    "    \n",
    "\n",
    "    # ------- Interaction features -------\n",
    "    feature_df['languages_x_statements'] = (\n",
    "        feature_df['log_num_languages'] * feature_df['log_num_statements']\n",
    "    )\n",
    "    feature_df['languages_x_page_length'] = (\n",
    "        feature_df['log_num_languages'] * feature_df['log_en_page_length']\n",
    "    )\n",
    "    feature_df['has_country_x_languages'] = (\n",
    "        feature_df['has_country'].astype(int) * feature_df['log_num_languages']\n",
    "    )\n",
    "    feature_df['has_country_x_statements'] = (\n",
    "        feature_df['has_country'].astype(int) * feature_df['log_num_statements']\n",
    "    )\n",
    "    feature_df['cultural_x_geographic'] = (\n",
    "        feature_df['num_cultural_properties'] * feature_df['num_geographic_properties']\n",
    "    )\n",
    "\n",
    "    # ------- Composite scores -------\n",
    "    feature_df['global_reach_score'] = (\n",
    "        feature_df['log_num_languages'] * 0.5 +\n",
    "        feature_df['log_en_page_length'] * 0.3 +\n",
    "        feature_df['log_num_external_links'] * 0.2\n",
    "    )\n",
    "\n",
    "    feature_df['cultural_specificity_score'] = (\n",
    "        feature_df['num_cultural_properties'] * 2.0 +\n",
    "        feature_df['num_geographic_properties'] * 1.5 +\n",
    "        feature_df['has_country'].astype(int) * 1.0 +\n",
    "        feature_df['has_origin_country'].astype(int) * 1.0 +\n",
    "        feature_df['has_culture_property'].astype(int) * 2.0\n",
    "    )\n",
    "\n",
    "    feature_df['info_richness_score'] = (\n",
    "        feature_df['log_num_statements'] * 0.4 +\n",
    "        feature_df['log_statement_diversity'] * 0.4 +\n",
    "        feature_df['log_num_identifiers'] * 0.2\n",
    "    )\n",
    "\n",
    "    feature_df['page_quality_score'] = (\n",
    "        feature_df['log_en_page_length'] * 0.4 +\n",
    "        feature_df['log_num_categories'] * 0.3 +\n",
    "        feature_df['log_num_external_links'] * 0.3\n",
    "    )\n",
    "\n",
    "    # ------- Binary thresholds -------\n",
    "    feature_df['is_highly_global']   = (feature_df['num_languages'] > 20).astype(int)\n",
    "    feature_df['is_niche']           = (feature_df['num_languages'] < 10).astype(int)\n",
    "    feature_df['has_long_page']      = (feature_df['en_page_length'] > 10000).astype(int)\n",
    "    feature_df['has_many_statements'] = (feature_df['num_statements'] > 30).astype(int)\n",
    "\n",
    "    # ------- Polynomial features -------\n",
    "    feature_df['num_languages_squared']        = feature_df['log_num_languages'] ** 2\n",
    "    feature_df['cultural_specificity_squared'] = feature_df['cultural_specificity_score'] ** 2\n",
    "\n",
    "    # EXACT feature order as training - 38 features\n",
    "    feature_cols = [\n",
    "        'log_num_languages', 'log_en_page_length', 'log_num_categories',\n",
    "        'log_num_external_links', 'log_num_statements', 'log_num_identifiers',\n",
    "        'log_statement_diversity', 'num_cultural_properties', 'num_geographic_properties',\n",
    "        'has_coordinates', 'has_country', 'has_culture_property', 'has_origin_country',\n",
    "        'cultural_ratio', 'geographic_ratio', 'identifier_ratio',\n",
    "        'categories_per_page', 'external_links_per_page',\n",
    "        'languages_x_statements', 'languages_x_page_length',\n",
    "        'has_country_x_languages', 'has_country_x_statements', 'cultural_x_geographic',\n",
    "        'global_reach_score', 'cultural_specificity_score',\n",
    "        'info_richness_score', 'page_quality_score',\n",
    "        'is_highly_global', 'is_niche', 'has_long_page', 'has_many_statements',\n",
    "        'num_languages_squared', 'cultural_specificity_squared'\n",
    "    ]\n",
    "\n",
    "    X = feature_df[feature_cols].fillna(0)\n",
    "    return X\n",
    "\n",
    "# Use it on the raw wiki features\n",
    "engineered_features = engineer_features(raw_features_df)\n",
    "print(\"Engineered features shape:\", engineered_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered features shape: (300, 33)\n",
      "Feature columns: 33\n"
     ]
    }
   ],
   "source": [
    "# Apply feature engineering\n",
    "features_df = engineer_features(raw_features_df)\n",
    "\n",
    "print(f\"Engineered features shape: {features_df.shape}\")\n",
    "print(f\"Feature columns: {len(features_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Load Model and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model from ArchitRastogi/CultureScope-XGBoost...\n",
      "Model downloaded to: /workspace/.cache/huggingface/hub/models--ArchitRastogi--CultureScope-XGBoost/snapshots/7ed86538361dcc5140c4f3bf938158bef072c7b9/best_model.pkl\n",
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Download model from HuggingFace Hub\n",
    "print(f\"Downloading model from {HF_MODEL_REPO}...\")\n",
    "model_path = hf_hub_download(repo_id=HF_MODEL_REPO, filename=MODEL_FILENAME)\n",
    "print(f\"Model downloaded to: {model_path}\")\n",
    "\n",
    "# Load the model\n",
    "model = joblib.load(model_path)\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions: ['cultural representative' 'cultural exclusive' 'cultural representative'\n",
      " 'cultural agnostic' 'cultural representative' 'cultural exclusive'\n",
      " 'cultural representative' 'cultural exclusive' 'cultural representative'\n",
      " 'cultural agnostic']\n",
      "\n",
      "Prediction distribution:\n",
      "label\n",
      "cultural agnostic          187\n",
      "cultural exclusive          67\n",
      "cultural representative     46\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Unpack model bundle (best_model.pkl / cultural_classifier_tuned.pkl)\n",
    "booster       = model[\"model\"]\n",
    "scaler        = model[\"scaler\"]\n",
    "label_encoder = model[\"label_encoder\"]\n",
    "feature_names = model[\"feature_names\"]\n",
    "\n",
    "# 1) Align engineered features with training feature order\n",
    "X = engineered_features[feature_names].fillna(0)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# 2) XGBoost prediction\n",
    "dtest = xgb.DMatrix(X_scaled, feature_names=feature_names)\n",
    "proba = booster.predict(dtest)\n",
    "\n",
    "# Class indices -> labels\n",
    "pred_indices = np.argmax(proba, axis=1)\n",
    "pred_labels = label_encoder.inverse_transform(pred_indices)\n",
    "\n",
    "print(\"Sample predictions:\", pred_labels[:10])\n",
    "\n",
    "# 3) Build dataframe with predictions\n",
    "pred_df = test_df.copy()\n",
    "\n",
    "# overwrite / create prediction column named 'label' (this is the predicted label)\n",
    "pred_df[\"label\"] = pred_labels\n",
    "\n",
    "print(\"\\nPrediction distribution:\")\n",
    "print(pred_df[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Save Predictions (Part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to: predictions.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>type</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.wikidata.org/entity/Q15786</td>\n",
       "      <td>1. FC Nürnberg</td>\n",
       "      <td>German sports club based in Nuremberg, Bavaria</td>\n",
       "      <td>entity</td>\n",
       "      <td>sports</td>\n",
       "      <td>sports club</td>\n",
       "      <td>cultural representative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    item            name  \\\n",
       "0  http://www.wikidata.org/entity/Q15786  1. FC Nürnberg   \n",
       "\n",
       "                                      description    type category  \\\n",
       "0  German sports club based in Nuremberg, Bavaria  entity   sports   \n",
       "\n",
       "   subcategory                    label  \n",
       "0  sports club  cultural representative  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns we want in the final CSV, in order\n",
    "output_columns = [\n",
    "    \"item\",\n",
    "    \"name\",\n",
    "    \"description\",\n",
    "    \"type\",\n",
    "    \"category\",\n",
    "    \"subcategory\",\n",
    "    \"label\",  # predicted label\n",
    "]\n",
    "\n",
    "# Keep only those columns (will error if any are missing, which is good to catch)\n",
    "final_pred_df = pred_df[output_columns]\n",
    "\n",
    "# Save to CSV\n",
    "final_pred_df.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "print(f\"Predictions saved to: {OUTPUT_CSV_PATH}\")\n",
    "final_pred_df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. FC Nürnberg</td>\n",
       "      <td>sports</td>\n",
       "      <td>cultural representative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77 Records</td>\n",
       "      <td>music</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Bug's Life</td>\n",
       "      <td>comics and anime</td>\n",
       "      <td>cultural representative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Gang Story</td>\n",
       "      <td>films</td>\n",
       "      <td>cultural agnostic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aaron Copland</td>\n",
       "      <td>performing arts</td>\n",
       "      <td>cultural representative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Aarwangen Castle</td>\n",
       "      <td>architecture</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>abaya</td>\n",
       "      <td>fashion</td>\n",
       "      <td>cultural representative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Academy of San Carlos</td>\n",
       "      <td>visual arts</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Africa</td>\n",
       "      <td>geography</td>\n",
       "      <td>cultural representative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>African American literature</td>\n",
       "      <td>literature</td>\n",
       "      <td>cultural agnostic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>agricultural irrigation</td>\n",
       "      <td>transportation</td>\n",
       "      <td>cultural agnostic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>aircraft</td>\n",
       "      <td>transportation</td>\n",
       "      <td>cultural agnostic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           name          category                    label\n",
       "0                1. FC Nürnberg            sports  cultural representative\n",
       "1                    77 Records             music       cultural exclusive\n",
       "2                  A Bug's Life  comics and anime  cultural representative\n",
       "3                  A Gang Story             films        cultural agnostic\n",
       "4                 Aaron Copland   performing arts  cultural representative\n",
       "5              Aarwangen Castle      architecture       cultural exclusive\n",
       "6                         abaya           fashion  cultural representative\n",
       "7         Academy of San Carlos       visual arts       cultural exclusive\n",
       "8                        Africa         geography  cultural representative\n",
       "9   African American literature        literature        cultural agnostic\n",
       "10      agricultural irrigation    transportation        cultural agnostic\n",
       "11                     aircraft    transportation        cultural agnostic"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display sample predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "final_pred_df[['name', 'category', 'label']].head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Without External API Calls\n",
    "\n",
    "This fallback approach uses a pre-trained ensemble model that relies on text-based features when external API access is not available. The model uses TF-IDF features from item metadata.\n",
    "\n",
    "**Note:** This approach requires the `ensemble_simple.pkl` file in the working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load Model and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model from ArchitRastogi/CultureScope-Ensemble-NoWiki...\n",
      "Model downloaded to: /workspace/.cache/huggingface/hub/models--ArchitRastogi--CultureScope-Ensemble-NoWiki/snapshots/430a4cbcfc660ac9cd51ce93dd6fecf83f71b4a9/ensemble_simple.pkl\n",
      "Loading ensemble model...\n",
      "Model loaded successfully.\n",
      "Output Path set to predictions_no_api.csv\n"
     ]
    }
   ],
   "source": [
    "# Download model from HuggingFace Hub\n",
    "print(f\"Downloading model from {HF_MODEL_REPO_PART2}...\")\n",
    "ENSEMBLE_MODEL_PATH = hf_hub_download(repo_id=HF_MODEL_REPO_PART2, filename=MODEL_FILENAME_PART2)\n",
    "print(f\"Model downloaded to: {ENSEMBLE_MODEL_PATH}\")\n",
    "\n",
    "print(\"Loading ensemble model...\")\n",
    "with open(ENSEMBLE_MODEL_PATH, 'rb') as f:\n",
    "    model_data = pickle.load(f)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "OUTPUT_CSV_PATH_PART2 = \"predictions_no_api.csv\"\n",
    "\n",
    "print(f\"Output Path set to {OUTPUT_CSV_PATH_PART2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load Ensemble Model and Prepare Feature Engineering Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "  Feature count: 29\n",
      "  Classes: ['cultural agnostic' 'cultural exclusive' 'cultural representative']\n",
      "  Ensemble weights: XGB=0.339, LGB=0.326, CAT=0.335\n",
      "\n",
      "Feature engineering function ready\n"
     ]
    }
   ],
   "source": [
    "# Extract model components\n",
    "xgb_model = model_data['xgb_model']\n",
    "lgb_model = model_data['lgb_model']\n",
    "catboost_model = model_data['catboost_model']\n",
    "scaler = model_data['scaler']\n",
    "label_encoder = model_data['label_encoder']\n",
    "feature_names = model_data['feature_names']\n",
    "ensemble_weights = model_data['ensemble_weights']\n",
    "\n",
    "print(\"Model loaded successfully\")\n",
    "print(f\"  Feature count: {len(feature_names)}\")\n",
    "print(f\"  Classes: {label_encoder.classes_}\")\n",
    "print(f\"  Ensemble weights: XGB={ensemble_weights['xgb']:.3f}, \"\n",
    "      f\"LGB={ensemble_weights['lgb']:.3f}, \"\n",
    "      f\"CAT={ensemble_weights['catboost']:.3f}\")\n",
    "\n",
    "\n",
    "def create_basic_features(df):\n",
    "    \"\"\"Create basic numerical features from available columns\"\"\"\n",
    "    print(\"Creating basic features from available data...\")\n",
    "    \n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # Text length features\n",
    "    features['name_length'] = df['name'].fillna('').astype(str).str.len()\n",
    "    features['desc_length'] = df['description'].fillna('').astype(str).str.len()\n",
    "    features['name_word_count'] = df['name'].fillna('').astype(str).str.split().str.len()\n",
    "    features['desc_word_count'] = df['description'].fillna('').astype(str).str.split().str.len()\n",
    "    \n",
    "    # Categorical encoding\n",
    "    features['type_entity'] = (df['type'] == 'entity').astype(int)\n",
    "    features['type_concept'] = (df['type'] == 'concept').astype(int)\n",
    "    \n",
    "    # Category features (one-hot top categories)\n",
    "    top_categories = ['music', 'films', 'sports', 'literature', 'visual arts', \n",
    "                      'architecture', 'media', 'history', 'politics', 'food']\n",
    "    for cat in top_categories:\n",
    "        features[f'cat_{cat.replace(\" \", \"_\")}'] = (df['category'] == cat).astype(int)\n",
    "    \n",
    "    # Default values for enriched features (zeros since model expects them)\n",
    "    enriched_cols = ['num_languages', 'en_page_length', 'num_statements', 'num_categories',\n",
    "                     'num_external_links', 'num_identifiers', 'statement_diversity',\n",
    "                     'num_cultural_properties', 'num_geographic_properties',\n",
    "                     'has_coordinates', 'has_country', 'has_culture_property', 'has_origin_country']\n",
    "    \n",
    "    for col in enriched_cols:\n",
    "        if col.startswith('has_'):\n",
    "            features[col] = 0\n",
    "        elif col.startswith('num_'):\n",
    "            features[col] = 1\n",
    "        else:\n",
    "            features[col] = 0\n",
    "    \n",
    "    # Fill NaN with 0\n",
    "    features = features.fillna(0)\n",
    "    \n",
    "    print(f\"  Created {len(features.columns)} features\")\n",
    "    return features\n",
    "\n",
    "print(\"\\nFeature engineering function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3  Make Predictions and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions on 300 samples...\n",
      "Creating basic features from available data...\n",
      "  Created 29 features\n",
      "Scaling features...\n",
      "Running ensemble prediction...\n",
      "Predictions complete\n",
      "\n",
      "Predictions saved to: predictions_no_api.csv\n",
      "\n",
      "Prediction distribution:\n",
      "label\n",
      "cultural agnostic          134\n",
      "cultural exclusive         124\n",
      "cultural representative     42\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. FC Nürnberg</td>\n",
       "      <td>sports</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77 Records</td>\n",
       "      <td>music</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Bug's Life</td>\n",
       "      <td>comics and anime</td>\n",
       "      <td>cultural representative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Gang Story</td>\n",
       "      <td>films</td>\n",
       "      <td>cultural representative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aaron Copland</td>\n",
       "      <td>performing arts</td>\n",
       "      <td>cultural representative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Aarwangen Castle</td>\n",
       "      <td>architecture</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>abaya</td>\n",
       "      <td>fashion</td>\n",
       "      <td>cultural agnostic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Academy of San Carlos</td>\n",
       "      <td>visual arts</td>\n",
       "      <td>cultural representative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Africa</td>\n",
       "      <td>geography</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>African American literature</td>\n",
       "      <td>literature</td>\n",
       "      <td>cultural exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>agricultural irrigation</td>\n",
       "      <td>transportation</td>\n",
       "      <td>cultural agnostic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>aircraft</td>\n",
       "      <td>transportation</td>\n",
       "      <td>cultural agnostic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           name          category                    label\n",
       "0                1. FC Nürnberg            sports       cultural exclusive\n",
       "1                    77 Records             music       cultural exclusive\n",
       "2                  A Bug's Life  comics and anime  cultural representative\n",
       "3                  A Gang Story             films  cultural representative\n",
       "4                 Aaron Copland   performing arts  cultural representative\n",
       "5              Aarwangen Castle      architecture       cultural exclusive\n",
       "6                         abaya           fashion        cultural agnostic\n",
       "7         Academy of San Carlos       visual arts  cultural representative\n",
       "8                        Africa         geography       cultural exclusive\n",
       "9   African American literature        literature       cultural exclusive\n",
       "10      agricultural irrigation    transportation        cultural agnostic\n",
       "11                     aircraft    transportation        cultural agnostic"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create basic features from test data\n",
    "print(f\"Making predictions on {len(test_df)} samples...\")\n",
    "X_test = create_basic_features(test_df)\n",
    "\n",
    "# Scale features\n",
    "print(\"Scaling features...\")\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Get predictions from all models\n",
    "print(\"Running ensemble prediction...\")\n",
    "dtest = xgb.DMatrix(X_test_scaled, feature_names=feature_names)\n",
    "\n",
    "xgb_pred_proba = xgb_model.predict(dtest)\n",
    "lgb_pred_proba = lgb_model.predict(X_test_scaled)\n",
    "cat_pred_proba = catboost_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Weighted ensemble\n",
    "ensemble_pred_proba = (\n",
    "    ensemble_weights['xgb'] * xgb_pred_proba +\n",
    "    ensemble_weights['lgb'] * lgb_pred_proba +\n",
    "    ensemble_weights['catboost'] * cat_pred_proba\n",
    ")\n",
    "\n",
    "# Get final predictions\n",
    "y_pred_encoded = np.argmax(ensemble_pred_proba, axis=1)\n",
    "y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "\n",
    "print(\"Predictions complete\")\n",
    "\n",
    "# Create output DataFrame with same format as Part 1\n",
    "output_df = test_df[['item', 'name', 'description', 'type', 'category', 'subcategory']].copy()\n",
    "output_df['label'] = y_pred\n",
    "\n",
    "# Add probability columns\n",
    "for i, class_label in enumerate(label_encoder.classes_):\n",
    "    output_df[f'prob_{class_label}'] = ensemble_pred_proba[:, i]\n",
    "\n",
    "# Save predictions\n",
    "output_df.to_csv(OUTPUT_CSV_PATH_PART2, index=False)\n",
    "print(f\"\\nPredictions saved to: {OUTPUT_CSV_PATH_PART2}\")\n",
    "\n",
    "# Show prediction distribution\n",
    "print(\"\\nPrediction distribution:\")\n",
    "print(output_df['label'].value_counts())\n",
    "\n",
    "# Display sample predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "display(output_df[['name', 'category', 'label']].head(12))\n",
    "\n",
    "# Set output_part2 for final summary cell\n",
    "output_part2 = OUTPUT_CSV_PATH_PART2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook provides two approaches for cultural specificity classification:\n",
    "\n",
    "| Approach | Model | Features | Output File |\n",
    "|----------|-------|----------|-------------|\n",
    "| Part 1 | XGBoost (Tuned) | Wikipedia/Wikidata | `predictions.csv` |\n",
    "| Part 2 | Ensemble | Text-only | `predictions_no_api.csv` |\n",
    "\n",
    "**Recommended:** Use Part 1 for best accuracy (F1: 0.6029)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "OUTPUT FILES\n",
      "==================================================\n",
      "\n",
      "Part 1 predictions: predictions.csv\n",
      "Part 2 predictions: predictions_no_api.csv\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Final output summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"OUTPUT FILES\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nPart 1 predictions: {OUTPUT_CSV_PATH}\")\n",
    "if output_part2 is not None:\n",
    "    print(f\"Part 2 predictions: {OUTPUT_CSV_PATH_PART2}\")\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on Validation Set\n",
    "\n",
    "### Part 1\n",
    "Number of samples: 300\n",
    "Accuracy: 0.6367\n",
    "Macro F1: 0.6029\n",
    "\n",
    "Per-class metrics:\n",
    "                         precision    recall  f1-score   support\n",
    "\n",
    "      cultural agnostic       0.60      0.96      0.74       117\n",
    "     cultural exclusive       0.64      0.57      0.60        76\n",
    "cultural representative       0.78      0.34      0.47       107\n",
    "\n",
    "               accuracy                           0.64       300\n",
    "              macro avg       0.67      0.62      0.60       300\n",
    "           weighted avg       0.68      0.64      0.61       300\n",
    "\n",
    "### Part 2 \n",
    "Number of samples: 300\n",
    "Accuracy: 0.5867\n",
    "Macro F1: 0.5558\n",
    "\n",
    "Per-class metrics:\n",
    "                         precision    recall  f1-score   support\n",
    "\n",
    "      cultural agnostic       0.68      0.78      0.73       117\n",
    "     cultural exclusive       0.47      0.76      0.58        76\n",
    "cultural representative       0.64      0.25      0.36       107\n",
    "\n",
    "               accuracy                           0.59       300\n",
    "              macro avg       0.60      0.60      0.56       300\n",
    "           weighted avg       0.61      0.59      0.56       300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script to Test Predictions vs Real value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "def load_data(valid_path: str, pred_path: str):\n",
    "    valid = pd.read_csv(valid_path)\n",
    "    preds = pd.read_csv(pred_path)\n",
    "\n",
    "    # Expect an `item` column to align rows + a `label` column\n",
    "    valid_labels = valid[[\"item\", \"label\"]].rename(columns={\"label\": \"true_label\"})\n",
    "    pred_labels = preds[[\"item\", \"label\"]].rename(columns={\"label\": \"pred_label\"})\n",
    "\n",
    "    merged = pd.merge(valid_labels, pred_labels, on=\"item\", how=\"inner\")\n",
    "\n",
    "    if len(merged) == 0:\n",
    "        raise ValueError(\"No overlapping `item` ids between valid.csv and predictions.csv.\")\n",
    "\n",
    "    if len(merged) != len(valid_labels):\n",
    "        print(\n",
    "            f\"Warning: only {len(merged)} / {len(valid_labels)} items from valid.csv \"\n",
    "            f\"were found in predictions.csv\"\n",
    "        )\n",
    "\n",
    "    return merged[\"true_label\"], merged[\"pred_label\"]\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Evaluate predictions against ground truth.\")\n",
    "    parser.add_argument(\"--valid\", default=\"valid.csv\", help=\"Path to valid.csv (ground truth)\")\n",
    "    parser.add_argument(\n",
    "        \"--predictions\", default=\"predictions.csv\", help=\"Path to predictions.csv\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    y_true, y_pred = load_data(args.valid, args.predictions)\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # F1 for 3-class (macro treats all classes equally)\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "    print(\"Number of samples:\", len(y_true))\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Macro F1: {f1_macro:.4f}\")\n",
    "    print(\"\\nPer-class metrics:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
